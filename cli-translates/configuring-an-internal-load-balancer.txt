LAB Configuring an Internal Load Balancer

in this LAB, we Create internal traffic and health check firewall rules
Create a NAT configuration using Cloud Router
Configure two instance templates
Create two managed instance groups
Configure and test an internal load balancer

# setting our environment

gcloud auth login

export PROJECT_ID=qwiklabs-gcp-02-54b1d05ed31b
export MY_REGION=us-central1
export ZONE_A=us-central1-a
export ZONE_B=us-central1-b

gcloud config set project $PROJECT_ID

Task 1. Configure internal traffic and health check firewall rules

# Exploring the network. we have two network ready in this demo. the default network and my-internal-app network. we will be using the my-internal-app network for this demo.

gcloud compute networks list 

gcloud compute networks subnets list --network=my-internal-app

# creating firewal-rule to allow internal traffic in my-internal-app network. both subnets in the network belong to 10.10.0.0/16 range. So the firewall rule will be configured to allow traffic from this range. 

gcloud compute firewall-rules create fw-allow-lb-access \
--network=my-internal-app \
--direction=INGRESS \
--priority=1000 \
--action=ALLOW \
--rules=all \
--source-ranges=10.10.0.0/16 \
--target-tags=backend-service

# creating firewall rule to allow health check

gcloud compute firewall-rules create fw-allow-health-checks \
--network=my-internal-app \
--direction=INGRESS \
--priority=1000 \
--action=ALLOW \
--rules=tcp:80 \
--source-ranges=130.211.0.0/22,35.191.0.0/16 \
--target-tags=backend-service

Task 2: Create a NAT configuration using Cloud Router

# Creating the Cloud Router

gcloud compute routers create nat-router-us-central1 \
--region=us-central1 \
--network=my-internal-app

# creating the nag gateway

gcloud compute routers nats create nat-config \
--router=nat-router-us-central1 \
--auto-allocate-nat-external-ips \
--nat-all-subnet-ip-ranges

Task 3. Configure instance templates and create instance groups

# Configuring the instance templates for each subnets

gcloud compute instance-templates create instance-template-1 \
--network=my-internal-app \
--subnet=subnet-a \
--tags=backend-service \
--no-address \
--metadata=startup-script-url=gs://cloud-training/gcpnet/ilb/startup.sh

gcloud compute instance-templates create instance-template-2 \
--network=my-internal-app \
--subnet=subnet-b \
--tags=backend-service \
--no-address \
--metadata=startup-script-url=gs://cloud-training/gcpnet/ilb/startup.sh

# Creating the managed instance groups 1

gcloud compute instance-groups managed create instance-group-1 \
--zone=us-central1-a \
--template=instance-template-1 \
--size=1

gcloud compute instance-groups managed set-autoscaling instance-group-1 \
--min-num-replicas=1 \
--max-num-replicas=5 \
--cool-down-period=45s \
--scale-based-on-cpu \
--target-cpu-utilization=0.8 \
--zone=us-central1-a

# creating the managed instance groups 2 

gcloud compute instance-groups managed create instance-group-2 \
--zone=us-central1-b \
--template=instance-template-2 \
--size=1

gcloud compute instance-groups managed set-autoscaling instance-group-2 \
--min-num-replicas=1 \
--max-num-replicas=5 \
--zone=us-central1-b \
--cool-down-period=45s \
--scale-based-on-cpu \
--target-cpu-utilization=0.8 \
--zone=us-central1-b

# Verifying the backends

gcloud compute instances list 

# creating a utility VM to access the backends' HTTP sites

gcloud compute instances create utility-vm \
--zone=us-central1-f \
--machine-type=f1-micro \
--image=debian-10-buster-v20200910 \
--image-project=debian-cloud \
--network=my-internal-app \
--subnet=subnet-a \
--private-network-ip=10.10.20.50 \
--no-address

# ssh into utility-vm

gcloud compute ssh utility-vm --zone=us-central1-f --tunnel-through-iap

#  verify the welcome page for instance-group-1-xxxx 

curl 10.10.20.2

# verify the welcome page for instance-group-2-xxxx

exit ssh

Task 4. Configure the internal load balancer

 1-step is to create a health check that will be used by the load balancer

gcloud compute health-checks create tcp my-ilb-health-check \
--port=80 \
--proxy-header=NONE \
--region=us-central1

 2-step we create the backend service

gcloud compute backend-services create my-ilb \
--load-balancing-scheme=internal \
--region=us-central1 \
--health-checks=my-ilb-health-check \
--network=my-internal-app \
--health-checks-region=us-central1

 3-step adding the instance groups to the backend service 

gcloud compute backend-services add-backend my-ilb \
--region=us-central1 \
--instance-group=instance-group-1 \
--instance-group-zone=us-central1-a

gcloud compute backend-services add-backend my-ilb \
--region=us-central1 \
--instance-group=instance-group-2 \
--instance-group-zone=us-central1-b

 4-step we need to reserve a private static ip for our frontend service.

gcloud compute addresses create my-ilb-ip \
--addresses=10.10.30.5 \
--region=us-central1 \
--subnet=subnet-b

 5-step creating the forwarding rule that will serve as our frontend 

gcloud compute forwarding-rules create my-ilb \
--backend-service=my-ilb \
--backend-service-region=us-central1 \
--load-balancing-scheme=internal \
--network=my-internal-app \
--subnet=subnet-b \
--address=10.10.30.5 \
--ip-protocol=tcp \
--ports=80 \
--region=us-central1

Task 5. Test the internal load balancer

Accessing the internal load balancer to verify that my-ilb IP address forwards traffic to instance-group-1 in us-central1-a and instance-group-2 in us-central1-b 

# ssh into utility-vm

gcloud compute ssh utility-vm --zone=us-central1-f --tunnel-through-iap

# verifying that the internal load balancer forwards traffic

curl 10.10.30.5

# running the above command several times, we should be able to see responses from instance-group-1 in us-central1-a and instance-group-2 in us-central1-b. That way we can see the traffic is balanced accross our vms in the backend service. 

--------END OF LAB-------------